---
title: "Exam"
author: "Angelo Capasso e Mariateresa Russo"
format: html
editor: visual
---
# DOMANDE
-PCA e MCA hanno dimensioni che racchiudono poca varianza/inertia spiegata
-
```{r setup, include = FALSE, echo = FALSE}
library(tidymodels)
library(tidyverse)
library(GGally)
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(Rdimtools)
library(readr)
library(janitor)
library(knitr)
library(patchwork)
library(gridExtra)
library(lattice)
library(plotly)
library(clustrd)
library(mclust)
library(cluster)
```

```{r, echo = FALSE, warning=FALSE, include=FALSE}
support2 <- read_csv("support2.csv")

data = support2
```

# Preprocessing 1

Ricodifica corretta per i nomi delle colonne

```{r, include=FALSE}
colnames(data) <- c("id", "age", "death", "sex", "hospdead", "slos", "d.time", "dzgroup", "dzclass", "num.co", "edu", "income", "scoma", "charges", "totcst", "totmcst", "avtisst", "race", "sps", "aps", "surv2m", "surv6m", "hday", "diabetes", "dementia", "ca", "prg2m", "prg6m", "dnr", "dnrday", "meanbp", "wblc", "hrt", "resp", "temp", "pafi", "alb", "bili", "crea", "sod", "ph", "glucose", "bun", "urine", "adlp", "adls", "sfdm2")

data |> 
  glimpse()
```

Prima ricodifica (type delle variabili) e scarto delle variabili derivate da modelli statistici

```{r}
data = data |>
  mutate(
    across(c(where(is.character), death, hospdead, 
                  diabetes, income, dementia), ~ factor(.x))
  ) |> 
  select(-c(id, scoma, sps, surv2m, surv6m, sfdm2))
```

# Esplorative Data Analysis

## Missing values

Conteggio dei valori mancanti

```{r}
missing_summary <- tibble(
  variabile = names(data),
  n_na      = colSums(is.na(data)),
  perc_na   = colMeans(is.na(data)) * 100
) |>
  filter(n_na > 0) |>         
  arrange(desc(perc_na))

missing_summary

ggplot(missing_summary, aes(x = reorder(variabile, perc_na), y = perc_na)) +
  geom_col(fill = "#8FD4B8") +
  coord_flip() +
  labs(title = "Variabili con valori mancanti",
       x = "Variabile", y = "% NA")
```

Analisi dettagliata degli Na delle prime 4 variabili con piÃ¹ Na

```{r}
vars <- c("bun", "urine", "glucose", "adlp")

data |> 
  filter(death == "1") |> 
  summarise(across(all_of(vars), ~ sum(is.na(.)))) |> 
  pivot_longer(everything(),
               names_to = "variabile",
               values_to = "n_mancanti")
```

adlp Ã¨ una variabile compilata dal paziente: siccome presenta piÃ¹ del 60% di Na, si valuta l'esclusione

## Outlier detection

Outlier detection usando IQR Rule

```{r}
data |> 
  select(where(is.numeric)) |>
  map_df(function(x){
    Q1 <- quantile(x, 0.25, na.rm = TRUE)
    Q3 <- quantile(x, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    tibble(
      n_outlier = sum(x < Q1 - 1.5*IQR | x > Q3 + 1.5*IQR, na.rm = TRUE),
      perc_outlier = mean(x < Q1 - 1.5*IQR | x > Q3 + 1.5*IQR, na.rm = TRUE)*100
    )
  }) |>
  mutate(variabile = names(data |> select(where(is.numeric)))) |>
  select(variabile, everything()) |>
  arrange(desc(n_outlier))
```

Visualizzo la distribuzione delle variabili con circa il 10% di outliers

```
vars <- c("hday", "crea", "bili", "charges",
          "dnrday", "slos", "totcst", "totmcst")

plots_hist <- map(
  vars,
  ~ lattice::histogram(
      as.formula(paste("~", .x)),
      data = data,
      main = paste(.x),
      col = "#4BBAC3",
      border = "grey30"
    )
)

for(p in plots_hist) print(p)
```

## Numerical Features

### Correlation matrix

```{r}
num_data <- data[, sapply(data, is.numeric)]

corr_mat <- cor(num_data, use = "pairwise.complete.obs")

levelplot(
  corr_mat,
  scales = list(x = list(rot = 45)),
  xlab = "Variabili",
  ylab = "Variabili",
  main = "Heatmap delle correlazioni",
  col.regions = colorRampPalette(c("#2A5783", "white", "#F07F28"))(100)
)
```

### Density plot

```{r}
num_data %>%
  pivot_longer(cols = everything(), names_to = "variabile", values_to = "valore") %>%
  ggplot(aes(x = valore)) +
  geom_density(fill = "#8FD4B8", alpha = 0.6) +
  facet_wrap(~ variabile, scales = "free", ncol = 4) +  # griglia con 4 colonne
  labs(title = "Density plot delle variabili numeriche",
       x = "Value",
       y = "Density") 

```

Variabili come prg6m, prg2m sembrano avere distribuzioni discrete o con pochi valori distinti: si potrebbero trattare come categoriche

charges, totcst, totmcst mostrano code lunghe: candidate per trasformazioni logaritmiche

adlp, adls, num.co: mostrano piÃ¹ picchi

### Hierarchical Clustering

```{r}
dist_mat <- as.dist(1 - corr_mat)

hc <- hclust(dist_mat, method = "complete")

plot(hc, main = "Hierarchical Clustering")
```

## Categorical Features

```{r, warning=FALSE}
data %>%
  select(where(is.factor)) %>%
  pivot_longer(cols = everything(), names_to = "variabile", values_to = "valore") %>%
  ggplot(aes(x = valore, y = ..prop.., group = 1, fill = valor)) +
  geom_bar(fill = "#8FD4B8") +
  facet_wrap(~ variabile, scales = "free") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Distribuzione percentuale delle variabili categoriali",
       x = "Categoria", y = "Percentuale") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none",
        strip.text = element_text(size = 10))

```

# Preprocessing 2

## ANG------Manage missing values

Rimozione Variabili con molti valori mancanti (+ del 30%)

```{r}
data <- data |> select(-c(
  adlp, urine, glucose, bun, totmcst, alb, adls, bili, pafi, ph,
  prg2m, edu, prg6m, totcst))
```

Rimozione variabili molto correlate

```{r}
data <- data |> 
  select(-c(avtisst, dnrday))
# adls adlp
# totmcst totcst charges
# aps avtisst
# dnrday slos

tot_data = data
```
 Restanti variabili
```{r}
tibble(
  variabile = names(data),
  n_na      = colSums(is.na(data)),
  perc_na   = colMeans(is.na(data)) * 100
) |>
  filter(n_na > 0) |>         
  arrange(desc(perc_na))
```

Imputazione valori mancanti delle restanti variabili

```{r}
data <- data |>
  mutate(income = fct_explicit_na(income, na_level = "nd"))

############## fare modello supervised per predire i NA oppure rimuovere osservazioni
```

## Feature Scaling

```{r}
data_scaled = data |> 
  mutate(across(
    where(is.numeric),
    ~ as.numeric(scale(.x))
  ))
```

# Unsupervised Learning

## ðŸ‘Ž PCA + Kmeans
```{r, warning=FALSE, }
data_scaled <- data_scaled |> na.omit() # DA TOGLIERE

num_data_scaled <- data_scaled |>
  select(where(is.numeric))

num_data_red = data |> 
  select(where(is.numeric)) 

ggpairs(
  num_data_scaled,
  lower = list(
    continuous = wrap("points", size = 0.8, alpha = 0.6, colour = "darkorange")
  ),
  progress = FALSE   # <-- disattiva i messaggi di avanzamento
)
```

PCA, Scelta del numero ragionevole di componenti

```{r}
pca <- prcomp(num_data_scaled, scale. = FALSE)
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 25))
```

scelta del numero ottimale di gruppi da identificare tramite silhouette

```{r}
fviz_nbclust(pca$x[, 1:3], FUNcluster = kmeans, method = "silhouette")
```
algoritmo kmeans sulle prime 3 PC
```{r}
set.seed(6)  
km_res <- kmeans(pca$x[, 1:3], centers = 2, nstart = 25)

# Data frame con i primi 3 PC e cluster
pca_clusters <- as.data.frame(pca$x[, 1:3])
colnames(pca_clusters) <- c("PC1", "PC2", "PC3")
pca_clusters$cluster <- as.factor(km_res$cluster)

# Plot 3D con Plotly
library(plotly)

p3d <- plot_ly(
  data = pca_clusters,
  x = ~PC1,
  y = ~PC2,
  z = ~PC3,
  type = "scatter3d",
  mode = "markers",
  color = ~cluster,        # cluster come fattore
  colors = "Set2",         # palette di colori
  marker = list(size = 3)  # dimensione dei punti
) %>%
  layout(
    title = "3D K-means clustering sui primi 3 PC",
    scene = list(
      xaxis = list(title = "PC1"),
      yaxis = list(title = "PC2"),
      zaxis = list(title = "PC3")
    )
  )

p3d

```


## ðŸ‘Ž RKM (DA FARE tuning per ndim e nclus)

```{r}
res_cluspca <- num_data_scaled |> 
  cluspca(ndim = 6, nclus = 2, method = "RKM")

coords <- as.data.frame(res_cluspca$obscoord)
coords$cluster <- as.factor(res_cluspca$cluster)

ggpairs(coords, columns = 1:3, aes(color = cluster))

sil <- silhouette(res_cluspca$cluster, dist(res_cluspca$obscoord))
mean_sil <- mean(sil[, 3])
cat('\nsilhouette: ', mean_sil)

```

## ðŸ‘Ž FKM (DA FARE tuning per ndim e nclus)

```{r}
res_cluspca <- num_data_scaled |> 
  cluspca(ndim = 6, nclus = 2, method = "FKM")

coords <- as.data.frame(res_cluspca$obscoord)
coords$cluster <- as.factor(res_cluspca$cluster)

ggpairs(coords, columns = 1:3, aes(color = cluster))

sil <- silhouette(res_cluspca$cluster, dist(res_cluspca$obscoord))
mean_sil <- mean(sil[, 3])
cat('\nsilhouette: ', mean_sil)
```
Sembra non ci siano variabili che 'causano rumore'

## âœ…Kmeans
Variabili che agiscono con maggiore intensitÃ  sulla prima componente, la quale sembra discriminare due gruppi
Diagramma di dispersione tra: 1) frequenza cardiaca del paziente e pressione del sangue media del paziente (misurate al terzo giorno); 2) frequenza cardiaca del paziente e temperatura del paziente (misurate al terzo giorno); 3) pressione media del sangue del paziente e temperatura del paziente (misurate al terzo giorno);

```{r}
ggpairs(
  num_data_scaled |> select(hrt, meanbp, temp),
  lower = list(
    continuous = wrap("points", size = 0.8, alpha = 0.6, colour = "darkorange")
  )
)
```
Semplice K-means su queste 3 variabili piÃ¹ discriminanti

```{r}
data_scaled |> 
  select(meanbp, hrt, temp) |> 
  na.omit() |> 
  fviz_nbclust(kmeans, method = "silhouette")

data_scaled |>
  select(meanbp, hrt, temp) |>
  na.omit() |>
  (\(d3) {
    km <- kmeans(d3, centers = 8, nstart = 200)
   if (FALSE) {
    p2d <- fviz_cluster(
    km,
    data = d3,
    geom = "point",
    ellipse = FALSE,
    label = "none",
    stand = FALSE,
    axes = c(1,2),
    xlab = "meanbp",
    ylab = "hrt",
    ggtheme = theme_minimal()
    )
  }
  # ---- PLOT 3D (plotly) ----
    p3d <- plotly::plot_ly(
      x = d3$meanbp,
      y = d3$hrt,
      z = d3$temp,
      type = "scatter3d",
      mode = "markers",
      color = factor(km$cluster),
      colors = "Set1",
      marker = list(size = 3)
    ) |>
      layout(
        title = "3D K-means clustering (meanbp, hrt, temp)",
        scene = list(
          xaxis = list(title = "meanbp"),
          yaxis = list(title = "hrt"),
          zaxis = list(title = "temp")
        )
      )
    p3d
  })()
```

## ðŸ‘Žâœ…GMM
```{r}
GMM = Mclust(num_data_scaled, G = 4, modelNames = "EII")
```

```{r}
data_scaled |>
  # 1. Selezione variabili numeriche e rimozione NA
  select(where(is.numeric)) |>
  na.omit() |>
  (\(df_num) {
    pca_res <- PCA(df_num, ncp = 4, graph = FALSE)
    pcs <- as.data.frame(pca_res$ind$coord[, 1:2])
    
    p1 <- fviz_nbclust(pcs, kmeans, method = "wss") + ggtitle("Elbow")
    p2 <- fviz_nbclust(pcs, kmeans, method = "silhouette") + ggtitle("Silhouette")
    p3 <- fviz_nbclust(pcs, kmeans, method = "gap_stat") + ggtitle("Gap Statistic")
    
    print(p1); print(p2); print(p3)
    
    km <- kmeans(pcs, centers = 2, nstart = 100)
    
    fviz_cluster(
      km,
      data = pcs,
      geom = "point",
      label = "none",
      ellipse = FALSE,
      ggtheme = theme_minimal()
    )
  })()
```

## âœ…MCA + Kmeans

mca con 5 componenti
```{r, warnings=FALSE}
cat_data <- data |> select(where(is.factor))

res_mca <- MCA(cat_data, ncp = 5, graph = TRUE)
fviz_screeplot(res_mca, addlabels = TRUE)
```

Scelta del numero ottimale di cluster (silhouette)
```{r}
fviz_nbclust(res_mca$ind$coord, kmeans, method = "silhouette")
```
K-means sui primi 3 assi MCA
```{r}
set.seed(6)
km_res <- kmeans(res_mca$ind$coord[, 1:3], centers = 4, nstart = 25)

mca_clusters <- as.data.frame(res_mca$ind$coord[, 1:3])
colnames(mca_clusters) <- c("Dim1", "Dim2", "Dim3")
mca_clusters$cluster <- as.factor(km_res$cluster)

p3d <- plot_ly(
  data = mca_clusters,
  x = ~Dim1,
  y = ~Dim2,
  z = ~Dim3,
  type = "scatter3d",
  mode = "markers",
  color = ~cluster,
  colors = "Set2",
  marker = list(size = 3)
) %>%
  layout(
    title = "3D K-means clustering su MCA",
    scene = list(
      xaxis = list(title = "Dim1"),
      yaxis = list(title = "Dim2"),
      zaxis = list(title = "Dim3")
    )
  )

p3d

sil <- silhouette(km_res$cluster, dist(res_mca$ind$coord[, 1:3]))
mean_sil <- mean(sil[, 3])
cat("\nsilhouette:", mean_sil)

xyplot(
  Dim3 ~ Dim1,
  data = mca_clusters,
  groups = cluster,
  auto.key = list(columns = 4, title = "Cluster"),
  par.settings = list(superpose.symbol = list(pch = 19, col = RColorBrewer::brewer.pal(4, "Set2"))),
  xlab = "Dimensione 1",
  ylab = "Dimensione 3",
  main = "K-means clustering su MCA (Dim1 vs Dim3)"
)

```


## ------ Compare clusterings (numerical and categorical)
```{r}
# K-means numerico sulle 3 variabili 
set.seed(6)
num_clusters <- data_scaled |>
  select(meanbp, hrt, temp) |>
  na.omit() |>
  (\(d3) kmeans(d3, centers = 8, nstart = 50))()

# MCA + K-means sulle variabili categoriche 
cat_data <- data |> select(where(is.factor))
res_mca <- MCA(cat_data, ncp = 5, graph = FALSE)

set.seed(6)
mca_clusters <- kmeans(res_mca$ind$coord[, 1:3], centers = 4, nstart = 50)

# Costruzione dataframe con entrambe le assegnazioni 
df_clusters <- data.frame(
  id = 1:nrow(data),  # indice osservazioni
  cluster_num = factor(num_clusters$cluster),
  cluster_mca = factor(mca_clusters$cluster)
)

head(df_clusters)

# Matrice di confusione tra le due colonne 
conf_mat <- table(df_clusters$cluster_num, df_clusters$cluster_mca)
conf_mat

```

## ------- MCA+numeriche +Kmeans










# Supervised Learning
```{r}
head(tot_data)
```

```{r}
ggplot(tot_data, aes(x = death, y = ..prop.., group = 1)) +
  geom_bar(fill = "#8FD4B8", color = "grey30") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Distribuzione della variabile target 'death'",
    x = "Death",
    y = "Percentuale"
  ) +
  theme_minimal(base_size = 14)
```

boxplot variabili numeriche per le classi della target
```{r}
tot_data |>
  pivot_longer(
    cols = where(is.numeric),
    names_to = "variabile",
    values_to = "valore"
  ) |>
  ggplot(aes(x = death, y = valore, fill = death)) +
  geom_boxplot(alpha = 0.7, outlier.color = "grey40") +
  facet_wrap(~ variabile, scales = "free", ncol = 4) +
  scale_fill_manual(values = c("#8FD4B8", "#F07F28")) +
  labs(
    title = "Boxplot delle variabili numeriche per categorie di death",
    x = "Death",
    y = "Valore"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    strip.text = element_text(size = 10),
    legend.position = "none"
  )
```

chi-quadro test tra target e categoriche (1 vs 1)
```{r}
cat_vars <- tot_data |> select(where(is.factor))

# Ciclo su tutte le variabili categoriche (escludendo death stessa)
chi_results <- map_df(
  setdiff(names(cat_vars), "death"),
  function(var) {
    tbl <- table(tot_data$death, cat_vars[[var]])
    test <- chisq.test(tbl)
    tibble(
      variabile = var,
      statistic = test$statistic,
      p_value   = test$p.value
    )
  }
)

# Ordina per significativitÃ 
chi_results |> arrange(p_value)
```

## Feature Selection
```{r}

```

## Feature scaling
```{r}
tot_data_scaled = tot_data |> 
  mutate(across(
    where(is.numeric),
    ~ as.numeric(scale(.x))))

head(tot_data_scaled)
```

## Unbalanced class
```{r}

```

## SVM
```{r}
set.seed(6)

split <- initial_split(data_scaled, prop = 0.8, strata = death)
train <- training(split)
test  <- testing(split)

folds <- vfold_cv(train, v = 10, strata = death)
```


```{r}
svm_rec <- recipe(death ~ ., data = train) |>
  step_impute_median(all_numeric_predictors()) |>   # imputazione numeriche
  step_impute_mode(all_nominal_predictors()) |>     # imputazione categoriche
  step_nzv(all_predictors()) |>                     # rimozione variabili quasi costanti
  step_dummy(all_nominal_predictors()) |>           # dummy encoding
  step_normalize(all_numeric_predictors())
  # se target sbilanciato: step_smote(death)

svm_spec <- svm_rbf(
  mode = "classification",
  cost = tune(),        # parametro C
  rbf_sigma = tune()    # parametro gamma
) |> 
  set_engine("kernlab")

svm_wf <- workflow() |>
  add_model(svm_spec) |>
  add_recipe(svm_rec)
```


tuning
```{r}
svm_params <- parameters(svm_spec)
svm_grid <- expand.grid(
  cost = c(0.1, 1, 10, 100, 1000),
  rbf_sigma = c(0.001, 0.01, 0.1, 1, 10)
)

# Tuning con cross-validation 
svm_tuned <- tune_grid(
  svm_wf,
  resamples = folds,
  grid = svm_grid,
  metrics = metric_set(accuracy, roc_auc, precision, recall, f_meas),
  control = control_grid(save_pred = TRUE)
)

collect_metrics(svm_tuned) |> arrange(desc(mean))
```

best model
```{r}
best_params <- select_best(svm_tuned, metric = "roc_auc")
svm_final_wf <- finalize_workflow(svm_wf, best_params)

svm_final_fit <- last_fit(svm_final_wf, split)

collect_metrics(svm_final_fit)

svm_final_fit |> 
  collect_predictions() |> 
  conf_mat(truth = death, estimate = .pred_class)

```

## Interpretability









# Idee IODICE
# Supervised Learning
Dopo il clustering fare Decision Tree o RF con feature Importance, Fatto con variabile target: cluster

# Unsupervised Learning
Dopo la classificazione, Fare clustering sui valori di Shapley
