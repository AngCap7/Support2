---
title: "Exam"
author: "Angelo Capasso e Mariateresa Russo"
format: html
editor: visual
---

```{r setup, include = FALSE, echo = FALSE}
library(tidymodels)
library(tidyverse)
library(GGally)
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(Rdimtools)
library(readr)
library(janitor)
library(knitr)
library(patchwork)
library(gridExtra)
library(lattice)
library(plotly)
library(clustrd)
library(mclust)
library(cluster)
library(VIM)
library(rpart)
library(kknn)
library(dplyr)
library(tidyr)
library(purrr)
library(rattle)
library(iml)
library(lime)
library(pheatmap)
library(treemapify)
library(plotly)
library(ranger)
library(yardstick)
```

```{r, echo = FALSE, warning=FALSE, include=FALSE}
support2 <- read_csv("support2.csv")

data = support2
```

# Preprocessing 1

Ricodifica corretta per i nomi delle colonne

```{r, include=FALSE}
colnames(data) <- c("id", "age", "death", "sex", "hospdead", "slos", "d.time", "dzgroup", "dzclass", "num.co", "edu", "income", "scoma", "charges", "totcst", "totmcst", "avtisst", "race", "sps", "aps", "surv2m", "surv6m", "hday", "diabetes", "dementia", "ca", "prg2m", "prg6m", "dnr", "dnrday", "meanbp", "wblc", "hrt", "resp", "temp", "pafi", "alb", "bili", "crea", "sod", "ph", "glucose", "bun", "urine", "adlp", "adls", "sfdm2")

support2_na_analysis = data

data |> 
  glimpse()
```

Prima ricodifica (type delle variabili) e scarto delle variabili derivate da modelli statistici

```{r}
data = data |>
  mutate(
    across(c(where(is.character), death, hospdead, 
                  diabetes, income, dementia), ~ factor(.x))
  ) |> 
  select(-c(id, scoma, sps, surv2m, surv6m, sfdm2))
```

# Esplorative Data Analysis

## Missing values

Conteggio dei valori mancanti

```{r}
missing_summary <- tibble(
  variabile = names(data),
  n_na      = colSums(is.na(data)),
  perc_na   = colMeans(is.na(data)) * 100
) |>
  filter(n_na > 0) |>         
  arrange(desc(perc_na))

missing_summary

ggplot(missing_summary, aes(x = reorder(variabile, perc_na), y = perc_na)) +
  geom_col(fill = "#8FD4B8") +
  coord_flip() +
  labs(title = "Variabili con valori mancanti",
       x = "Variabile", y = "% NA")
```

Analisi dettagliata degli Na delle prime 4 variabili con più Na

```{r}
vars <- c("bun", "urine", "glucose", "adlp")

data |> 
  filter(death == "1") |> 
  summarise(across(all_of(vars), ~ sum(is.na(.)))) |> 
  pivot_longer(everything(),
               names_to = "variabile",
               values_to = "n_mancanti")
```

adlp è una variabile compilata dal paziente: siccome presenta più del 60% di Na, si valuta l'esclusione

## Outlier detection

Outlier detection usando IQR Rule

```{r}
data |> 
  select(where(is.numeric)) |>
  map_df(function(x){
    Q1 <- quantile(x, 0.25, na.rm = TRUE)
    Q3 <- quantile(x, 0.75, na.rm = TRUE)
    IQR <- Q3 - Q1
    tibble(
      n_outlier = sum(x < Q1 - 1.5*IQR | x > Q3 + 1.5*IQR, na.rm = TRUE),
      perc_outlier = mean(x < Q1 - 1.5*IQR | x > Q3 + 1.5*IQR, na.rm = TRUE)*100
    )
  }) |>
  mutate(variabile = names(data |> select(where(is.numeric)))) |>
  select(variabile, everything()) |>
  arrange(desc(n_outlier))
```

Visualizzo la distribuzione delle variabili con circa il 10% di outliers

```{r}
vars <- c("hday", "crea", "bili", "charges",
          "dnrday", "slos", "totcst", "totmcst")

data_num <- data[, vars]

box_plots <- list()
density_plots <- list()
for(v in vars){
  box_plots[[v]] <- bwplot(~ get(v), data = data,
                                    main = paste("Density plot of", v),
                                    xlab = v,
                                    col = "steelblue",
                                    plot.points = FALSE)
}
for(p in box_plots) print(p)


box_plots <- list()
for(v in vars){
  density_plots[[v]] <- densityplot(~ get(v), data = data,
                                    main = paste("Density plot of", v),
                                    xlab = v,
                                    col = "steelblue",
                                    plot.points = FALSE)
}
for(p in density_plots) print(p)
```

## Numerical Features

### Correlation matrix

```{r}
num_data <- data[, sapply(data, is.numeric)]

corr_mat <- cor(num_data, use = "pairwise.complete.obs")

levelplot(
  corr_mat,
  scales = list(x = list(rot = 45)),
  xlab = "Variabili",
  ylab = "Variabili",
  main = "Heatmap delle correlazioni",
  col.regions = colorRampPalette(c("#2A5783", "white", "#F07F28"))(100)
)
```

### Density plot

```{r}
num_data %>%
  pivot_longer(cols = everything(), names_to = "variabile", values_to = "valore") %>%
  ggplot(aes(x = valore)) +
  geom_density(fill = "#8FD4B8", alpha = 0.6) +
  facet_wrap(~ variabile, scales = "free", ncol = 4) +  # griglia con 4 colonne
  labs(title = "Density plot delle variabili numeriche",
       x = "Value",
       y = "Density") 

```

Variabili come prg6m, prg2m sembrano avere distribuzioni discrete o con pochi valori distinti: si potrebbero trattare come categoriche

charges, totcst, totmcst mostrano code lunghe: candidate per trasformazioni logaritmiche

adlp, adls, num.co: mostrano più picchi

### Hierarchical Clustering

```{r}
dist_mat <- as.dist(1 - corr_mat)

hc <- hclust(dist_mat, method = "complete")

plot(hc, main = "Hierarchical Clustering")
```

## Categorical Features

```{r, warning=FALSE}
data %>%
  select(where(is.factor)) %>%
  pivot_longer(cols = everything(), names_to = "variabile", values_to = "valore") %>%
  ggplot(aes(x = valore, y = ..prop.., group = 1, fill = valor)) +
  geom_bar(fill = "#8FD4B8") +
  facet_wrap(~ variabile, scales = "free") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Distribuzione percentuale delle variabili categoriali",
       x = "Categoria", y = "Percentuale") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none",
        strip.text = element_text(size = 10))

```

# Preprocessing 2

## Manage missing values

Rimozione Variabili con molti valori mancanti (+ del 30%)

```{r}
data <- data |> select(-c(
  adlp, urine, glucose, bun, totmcst, alb, adls, bili, pafi, ph,
  prg2m, edu, prg6m, totcst))
```

Rimozione variabili molto correlate

```{r}
data <- data |> 
  select(-c(avtisst, dnrday))
# adls adlp
# totmcst totcst charges
# aps avtisst
# dnrday slos

tot_data = data
```

Restanti variabili

```{r}
tibble(
  variabile = names(data),
  n_na      = colSums(is.na(data)),
  perc_na   = colMeans(is.na(data)) * 100
) |>
  filter(n_na > 0) |>         
  arrange(desc(perc_na))
```

Imputazione valori mancanti delle restanti variabili

```{r}
data <- data |>
  mutate(income = fct_explicit_na(income, na_level = "nd"),
        id = seq(1:nrow(data)))

set.seed(6)
data_imp <- kNN(data, variable = c("wblc","charges","crea","race","dnr"), k = 5)

```

```{r}
data_imp = data_imp |> 
  select(-c(wblc_imp, charges_imp, crea_imp, race_imp, dnr_imp))
```

## Feature Scaling

```{r}
data_scaled = data_imp |> 
  select(-id) |> 
  mutate(across(
    where(is.numeric),
    ~ as.numeric(scale(.x))
  )) |>
  mutate(id = data$id)

data_scaled <- data_scaled |> 
  na.omit() 

str(data_scaled)
```

```{r}
set.seed(46)

split <- initial_split(data_scaled, prop = 0.8, strata = death)
train <- training(split)
test  <- testing(split)

folds <- vfold_cv(train, v = 10, strata = death)
```
# Unsupervised Learning
#----------------------------------------------------------------
## ❌ PCA + Kmeans (tutte le variabili numeriche)

```{r, warning=FALSE, }
num_data_scaled <- data_scaled |>
  select(where(is.numeric)) |> 
  select(-id)

num_data_red = data |> 
  select(where(is.numeric))

ggpairs(
  num_data_scaled,
  lower = list(
    continuous = wrap("points", size = 0.8, alpha = 0.6, colour = "darkorange")
  ),
  progress = FALSE   # <-- disattiva i messaggi di avanzamento
)
```


PCA, Scelta del numero ragionevole di componenti

```{r}
pca <- prcomp(num_data_scaled, scale. = FALSE)
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 25))
```

scelta del numero ottimale di gruppi da identificare tramite silhouette

```{r}
fviz_nbclust(pca$x[, 1:3], FUNcluster = kmeans, method = "silhouette")
```

algoritmo kmeans sulle prime 3 PC

```{r}
set.seed(6)  
km_res <- kmeans(pca$x[, 1:3], centers = 2, nstart = 25)

# Data frame con i primi 3 PC e cluster
pca_clusters <- as.data.frame(pca$x[, 1:3])
colnames(pca_clusters) <- c("PC1", "PC2", "PC3")
pca_clusters$cluster <- as.factor(km_res$cluster)

# Plot 3D con Plotly
library(plotly)

p3d <- plot_ly(
  data = pca_clusters,
  x = ~PC1,
  y = ~PC2,
  z = ~PC3,
  type = "scatter3d",
  mode = "markers",
  color = ~cluster,        # cluster come fattore
  colors = "Set2",         # palette di colori
  marker = list(size = 3)  # dimensione dei punti
) %>%
  layout(
    title = "3D K-means clustering sui primi 3 PC",
    scene = list(
      xaxis = list(title = "PC1"),
      yaxis = list(title = "PC2"),
      zaxis = list(title = "PC3")
    )
  )

p3d

```

## ❌ RKM (3 variabili)

```{r}
res_cluspca <- num_data_scaled |>
  select(meanbp, hrt, temp) %>% 
  cluspca(ndim = 2, nclus = 4, method = "RKM")

coords <- as.data.frame(res_cluspca$obscoord)
coords$cluster <- as.factor(res_cluspca$cluster)

ggpairs(coords, columns = 1:2, aes(color = cluster))

sil <- silhouette(res_cluspca$cluster, dist(res_cluspca$obscoord))
mean_sil <- mean(sil[, 3])
cat('\nsilhouette: ', mean_sil)

```

## ❌ FKM (3 variabili)

```{r}
res_cluspca <- num_data_scaled |> 
  select(meanbp, hrt, temp) %>% 
  cluspca(ndim = 2, nclus = 4, method = "FKM")

coords <- as.data.frame(res_cluspca$obscoord)
coords$cluster <- as.factor(res_cluspca$cluster)

ggpairs(coords, columns = 1:3, aes(color = cluster))

sil <- silhouette(res_cluspca$cluster, dist(res_cluspca$obscoord))
mean_sil <- mean(sil[, 3])
cat('\nsilhouette: ', mean_sil)
```

Sembra non ci siano variabili che 'causano rumore'

## ❌Kmeans (3 variabili)

Variabili che agiscono con maggiore intensità sulla prima componente, la quale sembra discriminare due gruppi Diagramma di dispersione tra: 1) frequenza cardiaca del paziente e pressione del sangue media del paziente (misurate al terzo giorno); 2) frequenza cardiaca del paziente e temperatura del paziente (misurate al terzo giorno); 3) pressione media del sangue del paziente e temperatura del paziente (misurate al terzo giorno);

```{r}
ggpairs(
  num_data_scaled |> select(hrt, meanbp, temp),
  lower = list(
    continuous = wrap("points", size = 0.8, alpha = 0.6, colour = "darkorange")
  )
)
```

Semplice K-means su queste 3 variabili più discriminanti

```{r}
data_scaled |> 
  select(meanbp, hrt, temp) |> 
  fviz_nbclust(kmeans, method = "silhouette")

data_simple_kmeans = data_scaled |>
  select(meanbp, hrt, temp)

km_simple <- kmeans(data_simple_kmeans, centers = 8, nstart = 200)

p3d <- plotly::plot_ly(
      x = data_simple_kmeans$meanbp,
      y = data_simple_kmeans$hrt,
      z = data_simple_kmeans$temp,
      type = "scatter3d",
      mode = "markers",
      color = factor(km_simple$cluster),
      colors = "Set1",
      marker = list(size = 3)
    ) |>
      layout(
        title = "3D K-means clustering (meanbp, hrt, temp)",
        scene = list(
          xaxis = list(title = "meanbp"),
          yaxis = list(title = "hrt"),
          zaxis = list(title = "temp")
        )
      )

p3d
```

## ❌GMM (3 variabili)

```{r}
GMM = Mclust(num_data_scaled, G = 4, modelNames = "EII")
```

```{r}
data_scaled |>
  # 1. Selezione variabili numeriche e rimozione NA
  select(where(is.numeric)) |>
  select(-id) |> 
  (\(df_num) {
    pca_res <- PCA(df_num, ncp = 4, graph = FALSE)
    pcs <- as.data.frame(pca_res$ind$coord[, 1:2])

    p2 <- fviz_nbclust(pcs, kmeans, method = "silhouette") + ggtitle("Silhouette")
    print(p2)
    
    km <- kmeans(pcs, centers = 2, nstart = 100)
    
    fviz_cluster(
      km,
      data = pcs,
      geom = "point",
      label = "none",
      ellipse = FALSE,
      ggtheme = theme_minimal()
    )
  })()
```

## ❌MCA + Kmeans (tutte le variabili categoriche)

mca con 4 componenti

```{r, warnings=FALSE}
cat_data <- data_scaled |> select(where(is.factor))

res_mca <- MCA(cat_data, ncp = 4, graph = TRUE)
fviz_screeplot(res_mca, addlabels = TRUE)
```

Scelta del numero ottimale di cluster (silhouette)

```{r}
fviz_nbclust(res_mca$ind$coord, kmeans, method = "silhouette")
```

K-means sui primi 3 assi MCA

```{r}
set.seed(6)
km_res <- kmeans(res_mca$ind$coord[, 1:3], centers = 4, nstart = 25)

mca_clusters <- as.data.frame(res_mca$ind$coord[, 1:3])
colnames(mca_clusters) <- c("Dim1", "Dim2", "Dim3")
mca_clusters$cluster <- as.factor(km_res$cluster)

p3d <- plot_ly(
  data = mca_clusters,
  x = ~Dim1,
  y = ~Dim2,
  z = ~Dim3,
  type = "scatter3d",
  mode = "markers",
  color = ~cluster,
  colors = "Set2",
  marker = list(size = 3)
) %>%
  layout(
    title = "3D K-means clustering su MCA",
    scene = list(
      xaxis = list(title = "Dim1"),
      yaxis = list(title = "Dim2"),
      zaxis = list(title = "Dim3")
    )
  )

p3d

sil <- silhouette(km_res$cluster, dist(res_mca$ind$coord[, 1:3]))
mean_sil <- mean(sil[, 3])
cat("\nsilhouette:", mean_sil)

xyplot(
  Dim3 ~ Dim1,
  data = mca_clusters,
  groups = cluster,
  auto.key = list(columns = 4, title = "Cluster"),
  par.settings = list(superpose.symbol = list(pch = 19, col = RColorBrewer::brewer.pal(4, "Set2"))),
  xlab = "Dimensione 1",
  ylab = "Dimensione 3",
  main = "K-means clustering su MCA (Dim1 vs Dim3)"
)

```

## ✅❌MCA + Kmeans (3 numeriche categorizzate + tutte categoriche)
categorizzare le 3 numeriche e fare MCA globale 
```{r}
num_vars <- data_scaled |> select(meanbp, hrt, temp)

stats <- num_vars |> summarise(across(everything(), list(min=min, med=median, max=max))) |> as.list()

fuzzy_encoded <- num_vars |> mutate(
  meanbp_low  = pmax(0, (stats$meanbp_med - meanbp)/(stats$meanbp_med - stats$meanbp_min)),
  meanbp_mid  = pmax(0, 1 - abs(meanbp - stats$meanbp_med)/(stats$meanbp_max - stats$meanbp_min)),
  meanbp_high = pmax(0, (meanbp - stats$meanbp_med)/(stats$meanbp_max - stats$meanbp_med)),
  
  hrt_low  = pmax(0, (stats$hrt_med - hrt)/(stats$hrt_med - stats$hrt_min)),
  hrt_mid  = pmax(0, 1 - abs(hrt - stats$hrt_med)/(stats$hrt_max - stats$hrt_min)),
  hrt_high = pmax(0, (hrt - stats$hrt_med)/(stats$hrt_max - stats$hrt_med)),
  
  temp_low  = pmax(0, (stats$temp_med - temp)/(stats$temp_med - stats$temp_min)),
  temp_mid  = pmax(0, 1 - abs(temp - stats$temp_med)/(stats$temp_max - stats$temp_min)),
  temp_high = pmax(0, (temp - stats$temp_med)/(stats$temp_max - stats$temp_med))
) |> 
  select(-c(meanbp, temp, hrt))

meanbp_cat <- factor(c("low","mid","high")[max.col(fuzzy_encoded[,c("meanbp_low","meanbp_mid","meanbp_high")])])
hrt_cat    <- factor(c("low","mid","high")[max.col(fuzzy_encoded[,c("hrt_low","hrt_mid","hrt_high")])])
temp_cat   <- factor(c("low","mid","high")[max.col(fuzzy_encoded[,c("temp_low","temp_mid","temp_high")])])

categorical_vars <- data_scaled |> select_if(is.factor)
categorical_vars_ext <- categorical_vars |> 
  mutate(meanbp_cat = meanbp_cat,
         hrt_cat    = hrt_cat,
         temp_cat   = temp_cat) |> 
  select(-income)   # se vuoi escludere income

```

```{r}
#MCA
res.mca_vars <- MCA(categorical_vars_ext, graph = FALSE)
fviz_mca_var(res.mca_vars, repel = TRUE) 

#KMEANS
coords_mca <- res.mca_vars$ind$coord[,c(1,3)]
fviz_nbclust(coords_mca, kmeans, method = "silhouette")

set.seed(6)
res.mca_kmeans <- kmeans(coords_mca, centers = 4, nstart = 25)
fviz_cluster(res.mca_kmeans, data = coords_mca,
             geom = "point",
             labelsize = 4,
             main = "K-means + MCA")

```
interpretation of results
```{r}
clustered_data <- categorical_vars_ext %>%
  mutate(cluster = factor(res.mca_kmeans$cluster))

## 1. Dimensioni dei cluster
cluster_sizes <- clustered_data %>%
  count(cluster) %>%
  mutate(prop = round(n / sum(n), 3))

print(cluster_sizes)

## 2. Frequenze relative per ogni variabile categoriale
freq_rel <- function(var){
  clustered_data %>%
    count(cluster, !!sym(var)) %>%
    group_by(cluster) %>%
    mutate(prop = round(n/sum(n), 3)) %>%
    arrange(cluster, desc(prop))
}

# Esempi
freq_rel("meanbp_cat")
freq_rel("hrt_cat")
freq_rel("temp_cat")
freq_rel("sex")     # se presente
freq_rel("death")   # se presente

## 3. Funzione per barplot con frequenze relative
plot_cluster_var <- function(var){
  clustered_data %>%
    count(cluster, !!sym(var)) %>%
    group_by(cluster) %>%
    mutate(prop = n/sum(n)) %>%
    ggplot(aes(x = cluster, y = prop, fill = !!sym(var))) +
      geom_bar(stat = "identity", position = "dodge") +
      labs(title = paste("Distribuzione relativa di", var, "nei cluster"),
           y = "Frequenza relativa", x = "Cluster") +
      theme_minimal()
}
plot_cluster_var("meanbp_cat")
plot_cluster_var("hrt_cat")
plot_cluster_var("temp_cat")
```
nei cluster non si riesce ad identificare un profilo paziente ben discriminato
#---------WORKSPACE1---------------

## ✅✅FAMD
FAMD permette di analizzare insieme variabili quantitative e qualitative, trovando pattern e gruppi in modo bilanciato e visivamente interpretabile
Le variabili numeriche vengono trattate come in PCA (centrate e ridotte)
Le variabili categoriali vengono trattate come in MCA (codifica disgiuntiva)
Ogni variabile ha lo stesso peso, così numeriche e categoriali contribuiscono in modo equilibrato
```{r,warning=FALSE}
set.seed(6)
train <- train|> 
  select(-id) 

res_famd <- FAMD(train, ncp = 5, graph = FALSE)
res_famd$eig
fviz_screeplot(res_famd)

fviz_famd_var(res_famd, repel = TRUE, col.var = "#8FD4B8") +
  labs(title = "Correlation circle (FAMD)")

```

applicazione del kmeans con FAMD
```{r}
coords_famd <- res_famd$ind$coord  

fviz_nbclust(coords_famd, kmeans)
```

```{r}
set.seed(6)
famd_km_res <- kmeans(coords_famd, centers = 2, nstart = 25)
```

creazione del dataframe FAMD
```{r}
coords_famd = as.data.frame(coords_famd)

colnames(coords_famd) = c("Dim1", "Dim2", "Dim3", "Dim4", "Dim5")

coords_famd$cluster = famd_km_res$cluster
```


Grafico per visualizzare la divisione in clusters: pazienti con malattie oncologiche vs malattie cardiopolmonari
```{r}
ggplot(coords_famd, aes(x = Dim1, y = Dim2, color = factor(cluster))) +
  geom_point(size = 1, alpha = 0.7) +
  labs(title = "Scatterplot FAMD-Kmeans",
       x = "Dim 1",
       y = "Dim 2",
       color = "Cluster") +
  theme_minimal()
```
```{r}
plot_ly(
  data = coords_famd,
  x = ~Dim1,
  y = ~Dim2,
  z = ~Dim3,
  type = "scatter3d",
  mode = "markers",
  color = ~factor(cluster),
  colors = "Set2",
  marker = list(size = 3)
) %>%
  layout(
    title = "3D Scatterplot FAMD-Kmeans",
    scene = list(
      xaxis = list(title = "Dim 1"),
      yaxis = list(title = "Dim 2"),
      zaxis = list(title = "Dim 3")
    )
  )

```

Applicazione del metodo sul test set
```{r}
test <- test|> 
  select(-id) 
res_famd_test <- predict(res_famd, newdata = test)

coords_test <- as.data.frame(res_famd_test$coord)

colnames(coords_test) = c("Dim1", "Dim2", "Dim3", "Dim4", "Dim5")

# Assegna ogni individuo al cluster più vicino
test$cluster <- apply(coords_test, 1, function(x) {
  which.min(colSums((t(famd_km_res$centers) - x)^2))
})
```


Grafico FAMD-Kmeans su test set
```{r}
ggplot(coords_test, aes(x = Dim1 , y = Dim2 , color = factor(test$cluster))) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Scatterplot FAMD-Kmeans (Test set)",
       x = "Dim 1",
       y = "Dim 2",
       color = "Cluster") +
  theme_minimal()
```

```{r}
plot_ly(
  data = coords_test,
  x = ~Dim1,
  y = ~Dim2,
  z = ~Dim3,
  type = "scatter3d",
  mode = "markers",
  color = ~factor(test$cluster),
  colors = "Set2",
  marker = list(size = 3)
) %>%
  layout(
    title = "3D Scatterplot FAMD-Kmeans",
    scene = list(
      xaxis = list(title = "Dim 1"),
      yaxis = list(title = "Dim 2"),
      zaxis = list(title = "Dim 3")
    )
  )

```
Applicazione ranger cluster ~ tutte le variabili e stampa della feature importance
```{r}
#joined <- support2_na_analysis %>% inner_join(train %>% select(id), by = "id")
#common_cols <- intersect(colnames(support2_na_analysis), colnames(train))
#support_vars <- setdiff(colnames(support2_na_analysis), common_cols)
#joined$cluster = coords_famd$cluster
#data_RF <- joined %>% select(all_of(support_vars), cluster) |> select(-c(sfdm2, surv2m, sps, surv6m))
```

# Surrogate Model: Random Forest
```{r}
data_RF <- test
set.seed(6)
rf_model <- ranger(cluster ~ ., data = data_RF, mtry = 4, importance = "impurity_corrected")

importance_df <- data.frame(
  variable = names(rf_model$variable.importance),
  importance = rf_model$variable.importance
)


importance_df <- importance_df[order(importance_df$importance, decreasing = TRUE), ]
ggplot(head(importance_df, 20), aes(x = reorder(variable, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "#8FD4B8") +
  coord_flip() +
  labs(title = "Feature Importance",
       x = "Feature",
       y = "Importance") +
  theme_minimal()

```

## Patient Profile
```{r}

df <- test %>% mutate(cluster = factor(cluster))

vars <- c("ca","dzgroup","dzclass","aps","charges","hday","d.time","slos","death","num.co","hospdead")
num_vars <- vars[sapply(df[vars], is.numeric)]
cat_vars <- setdiff(vars, num_vars)

# Boxplot numeriche in griglia 2 righe x 3 colonne
plot_num <- df %>%
  select(cluster, all_of(num_vars)) %>%
  pivot_longer(-cluster, names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = cluster, y = value, fill = cluster)) +
  geom_boxplot(outlier.size = 0.6, alpha = 0.7) +
  facet_wrap(~ variable, scales = "free_y", ncol = 3, nrow = 2) +
  labs(title = "Boxplot variabili numeriche per cluster", x = "Cluster", y = NULL) +
  theme_minimal() +
  theme(legend.position = "none")

print(plot_num)

# Stampa le categoriali una per una (frequenze relative per cluster)
for(v in cat_vars){
  p <- df %>%
    select(cluster, !!sym(v)) %>%
    rename(category = !!sym(v)) %>%
    filter(!is.na(category)) %>%
    group_by(cluster, category) %>%
    summarise(n = n(), .groups = "drop") %>%
    group_by(cluster) %>%
    mutate(prop = n / sum(n)) %>%
    ggplot(aes(x = cluster, y = prop, fill = category)) +
    geom_col(position = position_fill(), color = "black") +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    labs(title = paste("Distribuzione di", v, "per cluster"),
         x = "Cluster", y = "Proporzione", fill = v) +
    theme_minimal() +
    theme(legend.position = "bottom")
  print(p)
}
```

Si riescono a distinguere due gruppi di pazienti:
il primo gruppo è associato alle malattie oncologiche, il secondo  è caratterizzato da pazienti con altre malattie

#Na evaluation
```{r}
set.seed(6)

split <- initial_split(data_scaled, prop = 0.8, strata = death)
train <- training(split)
test  <- testing(split)

joined <- support2_na_analysis %>% inner_join(train %>% select(id), by = "id")
common_cols <- intersect(colnames(support2_na_analysis), colnames(train))
support_vars <- setdiff(colnames(support2_na_analysis), common_cols)
joined$cluster = coords_famd$cluster
data_NA <- joined %>% select(all_of(support_vars), cluster) |> select(-c(sfdm2, surv2m, sps, surv6m))
# --- Step 1: Percentuali di NA per variabile e cluster ---
na_summary <- data_NA %>%
  group_by(cluster) %>%
  summarise(across(everything(), ~ mean(is.na(.)) * 100))

# Heatmap delle percentuali di NA
mat <- as.matrix(na_summary[,-1])
rownames(mat) <- paste("Cluster", na_summary$cluster)

pheatmap(mat,
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         main = "NA percentage by cluster")

```
Sembra che i NA siano missing at random
# IODICE

# Supervised Learning

```{r}
tot_data = data_imp
head(tot_data)
```

```{r}
ggplot(tot_data, aes(x = death, y = ..prop.., group = 1)) +
  geom_bar(fill = "#8FD4B8", color = "grey30") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Distribuzione della variabile target 'death'",
    x = "Death",
    y = "Percentuale"
  ) +
  theme_minimal(base_size = 14)
```

boxplot variabili numeriche per le classi della target

```{r}
tot_data |>
  pivot_longer(
    cols = where(is.numeric),
    names_to = "variabile",
    values_to = "valore"
  ) |>
  ggplot(aes(x = death, y = valore, fill = death)) +
  geom_boxplot(alpha = 0.7, outlier.color = "grey40") +
  facet_wrap(~ variabile, scales = "free", ncol = 4) +
  scale_fill_manual(values = c("#8FD4B8", "#F07F28")) +
  labs(
    title = "Boxplot delle variabili numeriche per categorie di death",
    x = "Death",
    y = "Valore"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    strip.text = element_text(size = 10),
    legend.position = "none"
  )
```

chi-quadro test tra target e categoriche (1 vs 1)

```{r}
cat_vars <- tot_data |> select(where(is.factor))

# Ciclo su tutte le variabili categoriche (escludendo death stessa)
chi_results <- map_df(
  setdiff(names(cat_vars), "death"),
  function(var) {
    tbl <- table(tot_data$death, cat_vars[[var]])
    test <- chisq.test(tbl)
    tibble(
      variabile = var,
      statistic = test$statistic,
      p_value   = test$p.value
    )
  }
)

# Ordina per significatività
chi_results |> arrange(p_value)
```

## Feature scaling
```{r}
tot_data_scaled = tot_data |> 
  mutate(across(
    where(is.numeric),
    ~ as.numeric(scale(.x)))) |> 
  drop_na()

head(tot_data_scaled)
```


```{r}
train_svm = train |> 
  select(-id)

test_svm = test |> 
  select(-id)
```


## SVM

```{r}
rec <- recipe(death ~ ., data = train_svm) |>                      
  step_dummy(all_nominal_predictors())           # dummy encodin                            
# Specifica modello SVM con kernel RBF
svm_spec <- svm_rbf(
  mode = "classification",
  cost = tune(),            # penalty sugli errori
  rbf_sigma = tune()        # parametro kernel RBF
) |> 
  set_engine("kernlab")

# Workflow
svm_wf <- workflow() |>
  add_model(svm_spec) |>
  add_recipe(rec)
```

tuning Grid Search prova tutte le combinazioni possibili di iperparametri in una griglia predefinita, mentre Bayesian Optimization non prova tutte le combinazioni: costruisce un modello probabilistico dello spazio dei parametri

```{r}
set.seed(6)

svm_grid <- tibble(
  cost = c(0.1, 1, 10, 100),
  rbf_sigma = c(0.001, 0.01, 0.1, 1)
)

# Step 1: tuning a griglia
svm_grid_res <- tune_grid(
  svm_wf,
  resamples = folds,
  grid = svm_grid,
  metrics = metric_set(recall),
  control = control_grid(save_pred = TRUE)
)

# Step 2: tuning bayesiano partendo dai risultati della griglia
svm_bayes <- tune_bayes(
  svm_wf,
  resamples = folds,
  metrics = metric_set(recall),
  initial = svm_grid_res,             
  iter = 3,
  control = control_bayes(verbose = TRUE, save_pred = TRUE)
)

# Migliori parametri
best_params <- select_best(svm_bayes, metric = "recall")
final_svm <- finalize_workflow(svm_wf, best_params)
```

best model

```{r}
svm_final_fit <- last_fit(final_svm, split)

svm_final_fit |> 
  collect_predictions() |> 
  conf_mat(truth = death, estimate = .pred_class)


preds <- svm_final_fit %>% collect_predictions()

ms <- yardstick::metric_set(yardstick::accuracy, yardstick::recall, yardstick::precision)

ms(preds, truth = death, estimate = .pred_class, event_level = "second")


```


## LR penalized


```{r}
# Specifica modello: logistic regression penalizzata
log_reg_spec <- logistic_reg(
  mode = "classification",
  penalty = tune(),       # parametro di penalizzazione (L1/L2)
  mixture = 1             # 1 = LASSO, 0 = Ridge, valori intermedi = Elastic Net
) |>
  set_engine("glmnet")

# Workflow
log_reg_wf <- workflow() |>
  add_model(log_reg_spec) |>
  add_recipe(rec)   # puoi riutilizzare la stessa ricetta
```


```{r}
log_reg_grid <- tibble(
  penalty = c(0.001, 0.01, 0.1, 1)
)

log_reg_grid_res <- tune_grid(
  log_reg_wf,
  resamples = folds,
  grid = log_reg_grid,
  metrics = metric_set(roc_auc),
  control = control_grid(save_pred = TRUE)
)

# Step 2: tuning bayesiano partendo dai risultati della griglia
log_reg_bayes <- tune_bayes(
  log_reg_wf,
  resamples = folds,
  metrics = metric_set(roc_auc),
  initial = log_reg_grid_res,
  iter = 3,
  control = control_bayes(verbose = TRUE, save_pred = TRUE)
)
```


```{r}
best_params_lr <- select_best(log_reg_bayes, metric = "roc_auc")
final_lr <- finalize_workflow(log_reg_wf, best_params_lr)
```


```{r}
lr_final_fit <- last_fit(final_lr, split)

collect_metrics(lr_final_fit)

lr_final_fit |> 
  collect_predictions() |> 
  conf_mat(truth = death, estimate = .pred_class)
```



```{r}
svm_test_preds <- collect_predictions(svm_final_fit)
lr_test_preds  <- collect_predictions(lr_final_fit)
```


```{r}
# Calcola ROC separatamente e aggiungi la colonna 'model'
svm_roc <- svm_test_preds %>%
  roc_curve(truth = death, .pred_0) %>%
  mutate(model = "SVM")

log_reg_roc <- lr_test_preds %>%
  roc_curve(truth = death, .pred_0) %>%
  mutate(model = "Logistic Regression")

# Confronto ROC
bind_rows(svm_roc, log_reg_roc) %>%
  autoplot() +
  aes(color = model) +
  labs(title = "Test ROC Curve: SVM vs Logistic Regression")

```

#---------WORKSPACE2---------
## Interpretability

```{r}
library(iml)

dplyr::glimpse(train)
```

svm
```{r}
svm_fit_model <- svm_final_fit %>% extract_fit_parsnip() %>% pluck("fit")

# Funzione di predizione (probabilità classe positiva)
svm_predict_fun <- function(model, newdata) {
  predict(model, newdata, type = "probabilities")[,2]
}

# Predictor per iml (usa i dati trasformati dalla ricetta!)
train_baked <- bake(prep(rec, training = train), new_data = train)

svm_pred <- Predictor$new(
  model = svm_fit_model,
  data  = train_baked %>% dplyr::select(-death),
  y     = train_baked$death,
  predict.function = svm_predict_fun
)
```

lr
```{r}
lr_fitted <- fit(final_lr, data = train)

lr_fit_model <- lr_fitted %>% extract_fit_parsnip()

coef_mat <- as.matrix(coef(lr_fit_model$fit, s = best_params_lr$penalty))

coef_table <- tibble(
  feature = rownames(coef_mat),
  coefficient = coef_mat[,1]
) %>% 
  filter(feature != "(Intercept)") %>%
  mutate(abs_coeff = abs(coefficient)) %>%
  arrange(desc(abs_coeff))

train_baked <- bake(prep(rec, training = train), new_data = train)

# Funzione di predizione che restituisce la probabilità della classe positiva
lr_predict_fun <- function(model, newdata) {
  as.numeric(predict(model, newx = as.matrix(newdata), type = "response"))
}

# Creazione dell'oggetto Predictor
lr_pred <- Predictor$new(
  model = lr_fit_model$fit,
  data  = train_baked %>% select(-death),
  y     = train_baked$death,
  predict.function = lr_predict_fun
)

```
## Feature Importance
svm
```{r}
svm_imp <- FeatureImp$new(
  svm_pred,
  loss = "ce",          # cross-entropy per classificazione
  compare = "difference"
)

svm_imp_df <- svm_imp$results %>%
  arrange(desc(importance))

ggplot(svm_imp_df, aes(x = reorder(feature, importance), y = importance)) +
  geom_col(fill = "#F07F28") +
  coord_flip() +
  theme_minimal(base_size = 14) +
  labs(
    title = "Permutation Feature Importance – SVM (RBF)",
    x = "Feature",
    y = "Importance (loss increase)"
  )
```
lr
```{r}
ggplot(coef_table, aes(x = reorder(feature, abs_coeff), y = abs_coeff)) +
  geom_col(fill = "#20B2AA") +
  coord_flip() +
  theme_minimal(base_size = 14) +
  labs(
    title = "Feature Importance – Logistic Regression",
    x = "Feature",
    y = "|Coefficient|"
  )

```

```{r}
p_lr + p_svm +
  plot_annotation(
    title = "Confronto Feature Importance: Logistic Regression vs SVM",
    theme = theme(plot.title = element_text(size = 18, face = "bold"))
  )
```

## ---PDP/ALE
svm
```{r}
# PDP su una variabile (es. "age")
pdp_age <- FeatureEffect$new(
  svm_pred,
  feature = "meanbp",   
  grid.size = 20,
  method = "pdp"
)

# Plot
pdp_age$plot() + theme_minimal()
```

lr
```{r}
pdp_meanbp <- FeatureEffect$new(
  lr_pred,
  feature = "meanbp",
  method = "pdp",
  grid.size = 20
)

# Plot PDP
pdp_meanbp$plot() +
  theme_minimal(base_size = 14) +
  labs(
    title = "Partial Dependence Plot – Logistic Regression",
    y = "Predicted probability",
    x = "meanbp"
  )
```

## ---SHAP

visualise observations classification on test set

svm
```{r}
# Probabilità predette classe positiva
svm_pred_probs <- predict(svm_fit_model, train_baked %>% select(-death), type = "probabilities")[,2]

# Predizione classi (0/1) usando soglia 0.5
svm_pred_class <- ifelse(svm_pred_probs >= 0.5, 1, 0)

# Creiamo il data frame con predizioni
preds_svm <- train_baked %>%
  mutate(
    .pred_prob = svm_pred_probs,
    .pred_class = factor(svm_pred_class, levels = c(0,1)),
    death = factor(death, levels = c(0,1))
  )

# Aggiungi colonna case_type
preds_svm <- preds_svm %>%
  mutate(
    case_type = case_when(
      death == 1 & .pred_class == 1 ~ "TP",
      death == 0 & .pred_class == 0 ~ "TN",
      death == 0 & .pred_class == 1 ~ "FP",
      death == 1 & .pred_class == 0 ~ "FN"
    )
  )

# Crea un indice esplicito
preds_svm <- preds_svm %>%
  mutate(row_index = 1:n())

# Ottieni gli indici per ciascun gruppo
tp_index <- preds_svm %>% filter(case_type == "TP") %>% pull(row_index)
tn_index <- preds_svm %>% filter(case_type == "TN") %>% pull(row_index)
fp_index <- preds_svm %>% filter(case_type == "FP") %>% pull(row_index)
fn_index <- preds_svm %>% filter(case_type == "FN") %>% pull(row_index)

# Visualizza
cat("TP: ", tp_index)
cat("\nTN: ", tn_index)
cat("\nFP: ", fp_index)
cat("\nFN: ", fn_index)

```

lr
```{r}

# Probabilità predette classe positiva (estrai lambda corretto)
pred_probs <- predict(
  lr_fit_model$fit, 
  newx = as.matrix(train_baked %>% select(-death)), 
  type = "response", 
  s = best_params_lr$penalty   # <- IMPORTANTE, seleziona il lambda scelto dal tuning
)

# pred_probs ora è una matrice n_righe x 1 → convertiamo in vettore
pred_probs <- as.numeric(pred_probs)

# Predizione classi (0/1) usando soglia 0.5
pred_class <- ifelse(pred_probs >= 0.5, 1, 0)

# Data frame con predizioni
preds_lr <- train_baked %>%
  mutate(
    .pred_prob = pred_probs,
    .pred_class = factor(pred_class, levels = c(0,1)),
    death = factor(death, levels = c(0,1))
  )

# Case type TP/TN/FP/FN
preds_lr <- preds_lr %>%
  mutate(
    case_type = case_when(
      death == 1 & .pred_class == 1 ~ "TP",
      death == 0 & .pred_class == 0 ~ "TN",
      death == 0 & .pred_class == 1 ~ "FP",
      death == 1 & .pred_class == 0 ~ "FN"
    )
  )

# Indici osservazioni per ciascun gruppo
tp_index <- which(preds_lr$case_type == "TP")
tn_index <- which(preds_lr$case_type == "TN")
fp_index <- which(preds_lr$case_type == "FP")
fn_index <- which(preds_lr$case_type == "FN")

cat("TP: ", tp_index)
cat("\nTN: ", tn_index)
cat("\nFP: ", fp_index)
cat("\nFN: ", fn_index)

```

svm
```{r}
library(ggplot2)

# Indici delle osservazioni da analizzare
#obs_index <- c(12, 134, 165, 170, 182, 207)
obs_index <- c(2,3,10)
# Calcolo Shapley values per ciascuna osservazione
shap_all <- lapply(obs_index, function(i) {
  x_test <- train_baked[i, -which(names(train_baked)=="death"), drop = FALSE]
  shap <- Shapley$new(svm_pred, x.interest = x_test)
  df <- shap$results
  df$obs_id <- i   # aggiungo l'indice per facet_wrap
  df
})

# Unisco in un unico data frame
shap_all_df <- do.call(rbind, shap_all)

# Grafico con facet_wrap
ggplot(shap_all_df, aes(x = feature, y = phi, fill = phi > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_fill_manual(values = c("TRUE" = "#F07F28", "FALSE" = "#20B2AA")) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Shapley values",
    x = "Features",
    y = "Contribute (phi)"
  ) +
  facet_wrap(~ obs_id, ncol = 2)

```

lr
```{r}
# Seleziona alcune osservazioni dal train (ad esempio 2 TP, 2 FP, 1 FN)
obs_index <- c(tp_index[1:2], fp_index[1:2], fn_index[1])

# Calcolo Shapley values
shap_all_lr <- lapply(obs_index, function(i) {
  x_test <- train_baked[i, -which(names(train_baked)=="death"), drop = FALSE]
  shap <- Shapley$new(lr_pred, x.interest = x_test)
  df <- shap$results
  df$obs_id <- i
  df
})

# Unisci in un unico data frame
shap_all_df_lr <- do.call(rbind, shap_all_lr)

# Grafico SHAP
ggplot(shap_all_df_lr, aes(x = feature, y = phi, fill = phi > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_fill_manual(values = c("TRUE" = "#20B2AA", "FALSE" = "#F07F28")) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Shapley values – Logistic Regression",
    x = "Features",
    y = "Contribution (phi)"
  ) +
  facet_wrap(~ obs_id, ncol = 2)

```

## ---LIME
svm
```{r}
library(lime)

# 1. Creo l'oggetto explainer
# Nota: la funzione lime richiede dati "raw" senza la colonna target
explainer_svm <- lime(
  x = train_baked %>% select(-death),  # dati senza target
  model = svm_fit_model,
  bin_continuous = TRUE  # opzionale, discretizza le variabili continue per LIME
)

# 2. Seleziono alcune osservazioni da spiegare
# Puoi usare gli indici già definiti prima (es: TP, FP, FN)
obs_to_explain <- train_baked[c(tp_index[1], fp_index[1], fn_index[1]), -which(names(train_baked) == "death")]

# 3. Genero le spiegazioni LIME
lime_results <- explain(
  x = obs_to_explain,
  explainer = explainer_svm,
  n_features = 5,        # numero di feature da mostrare
  n_labels = 1           # classi da spiegare (nel tuo caso positiva = 1)
)

# 4. Visualizzo i risultati
plot_features(lime_results) +
  labs(
    title = "LIME Explanation – SVM",
    x = "Feature contribution",
    y = "Observation"
  ) +
  theme_minimal(base_size = 14)


```

```{r}


```

svm
```{r}



```

lr
```{r}


```


# EXTRA 2: Unsupervised Learning

Dopo la classificazione supervisionata, Fare clustering sui valori di Shapley




















