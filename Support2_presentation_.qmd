---
title: "EXAM"
author: "Mariateresa Russo e Angelo Capasso"
format:
  revealjs:
    theme: solarized
    highlight: pygments
    center: true
    transition: fade
    background-transition: slide
    slide-number: true
    preview-links: true
    code-fold: true
    code-summary: "code"
---

```{r setup, include = FALSE, echo = FALSE}
library(tidymodels)
library(tidyverse)
library(GGally)
library(ggplot2)
library(FactoMineR)
library(factoextra)
library(Rdimtools)
library(readr)
library(janitor)
library(knitr)
library(patchwork)
library(gridExtra)
library(lattice)
library(plotly)
library(clustrd)
library(mclust)
library(cluster)
library(VIM)
library(rpart)
library(kknn)
library(dplyr)
library(tidyr)
library(purrr)
library(rattle)
library(iml)
library(lime)
library(pheatmap)
library(treemapify)
library(plotly)
library(ranger)
library(yardstick)
library(tibble)
library(kableExtra)
library(ggbeeswarm)  
```

```{r, echo = FALSE, warning=FALSE, include=FALSE}
support2 <- read_csv("support2.csv")

data = support2
```

```{r, echo = FALSE, warning=FALSE, include=FALSE}
colnames(data) <- c("id", "age", "death", "sex", "hospdead", "slos", "d.time", "dzgroup", "dzclass", "num.co", "edu", "income", "scoma", "charges", "totcst", "totmcst", "avtisst", "race", "sps", "aps", "surv2m", "surv6m", "hday", "diabetes", "dementia", "ca", "prg2m", "prg6m", "dnr", "dnrday", "meanbp", "wblc", "hrt", "resp", "temp", "pafi", "alb", "bili", "crea", "sod", "ph", "glucose", "bun", "urine", "adlp", "adls", "sfdm2")

support2_na_analysis = data

data <- data |>
  dplyr::mutate(
    across(c(where(is.character), death, hospdead, 
                  diabetes, income, dementia), ~ factor(.x))) |> 
  dplyr::select(-c(scoma, sps, surv2m, surv6m, sfdm2))
```

# 1) Dataset

## Numerical Features

::: {style="font-size: 37%;"}
-   **age**: age\
-   **slos**: length of hospital stay (days)\
-   **d.time**: follow-up time (days)
-   **hday**: number of hospital days up to the clinical event (discharge or death)
-   **num.co**: number of comorbidities\
-   **edu**: years of education\
-   **charges**: charges billed to the patient\
-   **totcst**: cost estimated by the RCC index\
-   **totmcst**: actual cost billed considering bonuses\
-   **avtisst**: Therapeutic Intervention Scoring System (intensity of care)\
-   **aps**: Acute Physiology Score (vital conditions assessment)\
-   **meanbp**: mean blood pressure\
-   **wblc**: white blood cell count\
-   **hrt**: heart rate\
-   **resp**: respiratory rate\
-   **temp**: body temperature\
-   **pafi**: PaO2/FiO2 ratio\
-   **alb**: albumin\
-   **bili**: bilirubin\
-   **crea**: creatinine\
-   **sod**: sodium\
-   **ph**: blood pH\
-   **glucose**, **bun**, **urine**\
-   **adlp**, **adls**: scores indicating daily living activities\
-   **dnrday**: day of the DNR (do-not-resuscitate) decision\
-   **prg2m**, **prg6m**: survival probability at 2 and 6 months\
:::

## Categorical Features

::: {style="font-size: 60%;"}
-   **death**: 0 = alive, 1 = deceased\
-   **sex**: male / female\
-   **hospdead**: death in hospital (0/1)\
-   **diabetes**, **dementia**: presence/absence\
-   **dzgroup**: diagnostic group (e.g., Lung Cancer, Cirrhosis, ARF/MOSF)\
-   **dzclass**: broader diagnostic class\
-   **income**: income category\
-   **race**: white, black, other\
-   **ca**: cancer category (none / local / metastatic)\
-   **dnr**: DNR status (do-not-resuscitate)\
-   **sfdm2**: functional status at 2 months
:::

# 2) Esplorative Data Analysis

## Missing values

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"
missing_summary <- tibble(
  variabile = names(data),
  n_na      = colSums(is.na(data)),
  perc_na   = colMeans(is.na(data)) * 100
) |>
  dplyr::filter(n_na > 0) |>         
  arrange(desc(perc_na))

ggplot(missing_summary, aes(x = reorder(variabile, perc_na), y = perc_na)) +
  geom_col(fill = "#8FD4B8") +
  coord_flip() +
  labs(title = "Features with missing values",
       x = "Feature", y = "% NA")
```

## Numerical Features

### Correlation matrix

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"
num_data <- data[, sapply(data, is.numeric)]

corr_mat <- cor(num_data, use = "pairwise.complete.obs")

levelplot(
  corr_mat,
  scales = list(x = list(rot = 45)),
  xlab = " ",
  ylab = " ",
  main = "Correlation Heatmap",
  col.regions = colorRampPalette(c("#2A5783", "white", "#F07F28"))(100)
)
```

### Hierarchical Clustering

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"
dist_mat <- as.dist(1 - corr_mat)

hc <- hclust(dist_mat, method = "complete")

plot(hc, main = "Hierarchical Clustering")
```

## Categorical Features

```{r, warning=FALSE}
#| echo: true
#| code-fold: true
#| code-summary: "code"
data %>%
  dplyr::select(where(is.factor)) %>%
  tidyr::pivot_longer(cols = everything(), names_to = "variabile", values_to = "valore") %>%
  ggplot(aes(x = valore, y = ..prop.., group = 1, fill = valor)) +
  geom_bar(fill = "#8FD4B8") +
  facet_wrap(~ variabile, scales = "free") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Categorical Features-frequency distribution",
       x = "Category", y = "Percentage") +
  theme_minimal(base_size = 12) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none",
        strip.text = element_text(size = 10))

```

## Manage Missing Values

::: {style="font-size: 50%;"}
-Remove features with more than 30% NA: bun, urine, glucose, adlp

```{r}
data <- data |> dplyr::select(-c(
  adlp, urine, glucose, bun, totmcst, alb, adls, bili, pafi, ph,
  prg2m, edu, prg6m, totcst))
```

-Remove features correlated with other features: avtisst, dnrday

```{r}
data <- data |> 
  dplyr::select(-c(avtisst, dnrday))
# adls adlp
# totmcst totcst charges
# aps avtisst
# dnrday slos
```

-Imputation for other features: income, wblc, charges, crea, race, dnr

```{r}
#| echo: false
#| code-fold: true
#| code-summary: "code"
#| 
data <- data |>
  dplyr::mutate(income = fct_explicit_na(income, na_level = "nd"))

library(VIM)
set.seed(6)
data_imp <- VIM::kNN(data, variable = c("wblc","charges","crea","race","dnr"), k = 5)
data_imp <- data_imp |> 
  dplyr::select(-c(wblc_imp, charges_imp, crea_imp, race_imp, dnr_imp, income, race))

#income,race
```
:::

```{r,echo=FALSE,include=FALSE}
data_scaled = data |>
  mutate(across(where(is.numeric) & !matches("id"),
    ~ as.numeric(scale(.x))
  ))

data_scaled <- data_scaled |> 
  na.omit() 

as_tibble(data_scaled)
```

```{r,echo=FALSE,include=FALSE}
set.seed(6)

split <- initial_split(data_scaled, prop = 0.8, strata = death)
train <- training(split)
test  <- testing(split)

folds <- vfold_cv(train, v = 10, strata = death)
```

# 3) Unsupervised Model : FAMD

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

set.seed(6)
#train <- train|> 
  #dplyr::select(-id) 

res_famd <- FAMD(train |> dplyr::select(-id), ncp = 5, graph = FALSE)

fviz_screeplot(res_famd)
```

## Optimal number of clusters

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

coords_famd <- res_famd$ind$coord  

fviz_nbclust(coords_famd, kmeans)
set.seed(6)
famd_km_res <- kmeans(coords_famd, centers = 2, nstart = 25)
```

## Clustering: Kmeans

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

coords_famd = as.data.frame(coords_famd)

colnames(coords_famd) = c("Dim1", "Dim2", "Dim3", "Dim4", "Dim5")

coords_famd$cluster = famd_km_res$cluster

plot_ly(
  data = coords_famd,
  x = ~Dim1,
  y = ~Dim2,
  z = ~Dim3,
  type = "scatter3d",
  mode = "markers",
  color = ~factor(cluster),
  colors = "Set2",
  marker = list(size = 3)
) %>%
  layout(
    title = "3D Scatterplot FAMD-Kmeans",
    scene = list(
      xaxis = list(title = "Dim 1"),
      yaxis = list(title = "Dim 2"),
      zaxis = list(title = "Dim 3")
    )
  )
```

## Evaluation of results

Clustering coerence is now evaluated on the test set

```{r,echo=FALSE,include=FALSE}
test_noid <- test|> 
  select(-id) 

res_famd_test <- predict(res_famd, newdata = test_noid)

coords_test <- as.data.frame(res_famd_test$coord)

colnames(coords_test) = c("Dim1", "Dim2", "Dim3", "Dim4", "Dim5")

test$cluster <- apply(coords_test, 1, function(x) {
  which.min(colSums((t(famd_km_res$centers) - x)^2))
})
```

### Random Forest and Feature importance

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

data_RF <- test
set.seed(6)
rf_model <- ranger(cluster ~ ., data = data_RF, mtry = 4, importance = "impurity_corrected")

importance_df <- data.frame(
  variable = names(rf_model$variable.importance),
  importance = rf_model$variable.importance)

importance_df <- importance_df[order(importance_df$importance, decreasing = TRUE), ]
ggplot(head(importance_df, 20), aes(x = reorder(variable, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "#8FD4B8") +
  coord_flip() +
  labs(title = "Feature Importance",
       x = "Feature",
       y = "Importance") +
  theme_minimal()
```

### Patient Profile for each cluster

```{r}
df <- test %>% mutate(cluster = factor(cluster))

vars <- c("ca","dzgroup","dzclass","aps","charges","hday","d.time","slos","death","num.co","hospdead")
num_vars <- vars[sapply(df[vars], is.numeric)]
cat_vars <- setdiff(vars, num_vars)

# Boxplot numeriche in griglia 2 righe x 3 colonne
plot_num <- df %>%
  select(cluster, all_of(num_vars)) %>%
  pivot_longer(-cluster, names_to = "variable", values_to = "value") %>%
  ggplot(aes(x = cluster, y = value, fill = cluster)) +
  geom_boxplot(outlier.size = 0.6, alpha = 0.7) +
  facet_wrap(~ variable, scales = "free_y", ncol = 3, nrow = 2) +
  labs(title = "Boxplot of numeric vars per cluster", x = "Cluster", y = NULL) +
  theme_minimal() +
  theme(legend.position = "none")

print(plot_num)

# Stampa le categoriali una per una (frequenze relative per cluster)
# Convertiamo le variabili categoriche in formato long
df_cat_long <- df %>%
  select(cluster, all_of(cat_vars)) %>%
  pivot_longer(
    cols = -cluster,
    names_to = "variable",
    values_to = "category"
  ) %>%
  filter(!is.na(category))  # rimuove NA

# Grafico a barre con proporzioni per cluster
ggplot(df_cat_long, aes(x = cluster, fill = category)) +
  geom_bar(position = "fill", color = "black") +
  facet_wrap(~ variable, scales = "free_y", ncol = 3) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  labs(
    title = "Distribuzione delle variabili categoriche per cluster",
    x = "Cluster",
    y = "Proporzione",
    fill = "Categoria"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

## NA evaluation

NAs seem to be missing at random

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

set.seed(6)

split <- initial_split(data_scaled, prop = 0.8, strata = death)
train <- training(split)
test  <- testing(split)

joined <- support2_na_analysis %>% inner_join(train %>% select(id), by = "id")
common_cols <- intersect(colnames(support2_na_analysis), colnames(train))
support_vars <- setdiff(colnames(support2_na_analysis), common_cols)
joined$cluster = coords_famd$cluster
data_NA <- joined %>% select(all_of(support_vars), cluster) |> select(-c(sfdm2, surv2m, sps, surv6m))
# Percentuali di NA per variabile e cluster ---
na_summary <- dplyr::summarise(
  dplyr::group_by(data_NA, cluster),
  dplyr::across(
    dplyr::everything(),
    ~ base::mean(base::is.na(.)) * 100))

# Heatmap delle percentuali di NA
mat <- as.matrix(na_summary[,-1])
rownames(mat) <- paste("Cluster", na_summary$cluster)

pheatmap(mat,
         cluster_rows = FALSE,
         cluster_cols = FALSE,
         main = "NA percentage by cluster")

```

## 4) Supervised Model: LR vs SVM

```{r,echo=FALSE,include=FALSE}
tot_data = data_imp
head(tot_data)

tot_data_scaled = tot_data |> 
  select(-id) |> 
  mutate(across(
    where(is.numeric),
    ~ as.numeric(scale(.x)))) |> 
  drop_na()
```

## The target variable

```{r}
ggplot(tot_data, aes(x = death, y = ..prop.., group = 1)) +
  geom_bar(fill = "#8FD4B8", color = "grey30") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "'death' Frequency",
    x = "Death",
    y = "Percentage"
  ) +
  theme_minimal(base_size = 14)
```

## SVM Model

::: {style="font-size: 50%;"}
-   Train Test split

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

set.seed(6)

split <- initial_split(tot_data_scaled, prop = 0.8, strata = death)
train <- training(split)
test  <- testing(split)

folds <- vfold_cv(train, v = 10, strata = death)
```

-   Recipe and Workflow

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

svm_rec <- recipe(death ~ ., data = train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())


svm_spec <- svm_linear(
  mode = "classification",
  cost = best_params_svm$cost
) %>%
  set_engine("kernlab")

# Workflow
svm_wf <- workflow() %>%
  add_recipe(svm_rec) %>%
  add_model(svm_spec)
```

-   Tuning parameters: cost and complexity(rbf_sigma)

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

set.seed(6)

svm_grid <- tibble(
  cost = c(0.1, 1, 10, 100),
  rbf_sigma = c(0.001, 0.01, 0.1, 1)
)

# Step 1: gridsearch
#svm_grid_res <- tune_grid(svm_wf, resamples = folds, grid = svm_grid, metrics = metric_set(recall))

# Step 2: bayesian tuning
#svm_bayes <- tune_bayes(svm_wf, resamples = folds, metrics = metric_set(recall), initial = svm_grid_res, iter = 3, control = control_bayes(verbose = TRUE))

# Best params
best_params_svm <- tibble::tibble(cost = 1) # rbf_sigma = 0.01) #auc
#best_params_svm <- tibble::tibble(cost = 3.37)#, rbf_sigma = 0.00857) #recall
final_svm <- finalize_workflow(svm_wf, best_params_svm)

best_params_svm
```

-   Results

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

svm_final_fit <- last_fit(svm_wf, split)
collect_predictions(svm_final_fit)

ms_svm <- yardstick::metric_set(yardstick::accuracy, yardstick::recall, yardstick::precision)

preds_svm <- collect_predictions(svm_final_fit)
ms_svm(preds_svm, truth = death, estimate = .pred_class, event_level = "second")
```
:::

## LR Model

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

# logistic regression penalized
log_reg_spec <- logistic_reg(
  mode = "classification",
  penalty = tune(),     
  mixture = 1             # 1 = LASSO, 0 = Ridge 
) |>
  set_engine("glmnet")

# Workflow
log_reg_wf <- workflow() |>
  add_model(log_reg_spec) |>
  add_recipe(svm_rec)   

# tuning
log_reg_grid <- tibble(
  penalty = c(0.001, 0.01, 0.1, 1)
)

#log_reg_grid_res <- tune_grid(log_reg_wf, resamples = folds, grid = log_reg_grid, metrics = metric_set(recall), control = control_grid(save_pred = TRUE))

#log_reg_bayes <- tune_bayes(log_reg_wf, resamples = folds, metrics = metric_set(recall), initial = log_reg_grid_res, iter = 3, control = control_bayes(verbose = TRUE, save_pred = TRUE))


best_params_lr <- tibble::tibble(penalty = 0.01) #auc
#best_params_lr <- tibble::tibble(penalty = 0.001) #recall
final_lr <- finalize_workflow(log_reg_wf, best_params_lr)

lr_final_fit <- last_fit(final_lr, split)

preds_lr  <- collect_predictions(lr_final_fit)

ms_lr <- yardstick::metric_set(yardstick::accuracy, yardstick::recall, yardstick::precision)

ms_lr(preds_lr, truth = death, estimate = .pred_class, event_level = "second")
```

## SVM vs LR: ROC curve

```{r}
#| echo: true
#| code-fold: false

svm_preds <- collect_predictions(svm_final_fit)
lr_preds  <- collect_predictions(lr_final_fit)

svm_roc <- svm_preds %>%
  roc_curve(truth = death, .pred_0) %>%
  dplyr::mutate(model = "SVM")

log_reg_roc <- lr_preds %>%
  roc_curve(truth = death, .pred_0) %>%
  mutate(model = "Logistic Regression")

bind_rows(svm_roc, log_reg_roc) %>%
  autoplot() +
  aes(color = model) +
  labs(title = "Test ROC Curve: SVM vs Logistic Regression")

```

```{r,warning=FALSE,include=FALSE}
metrics_svm <- ms_svm(preds_svm, truth = death, estimate = .pred_class, event_level = "second") %>% mutate(model = "SVM")
metrics_lr  <- ms_lr(preds_lr,  truth = death, estimate = .pred_class, event_level = "second") %>% mutate(model = "LR")

combined <- dplyr::bind_rows(metrics_svm, metrics_lr) %>%
  dplyr::select(model, .metric, .estimate) %>%
  pivot_wider(names_from = model, values_from = .estimate)
```


## SVM vs LR: Confusion matrix and metrics

```{r}
#| echo: true
#| code-fold: false

#confusion matrix
p_svm <- autoplot(
  preds_svm %>% conf_mat(truth = death, estimate = .pred_class),
  type = "heatmap"
) +
  scale_fill_gradient(low = "#ccece6", high = "#458B74") +
  ggtitle("Confusion Matrix – SVM") +
  theme_minimal(base_size = 14)

p_lr <- autoplot(
  preds_lr %>% conf_mat(truth = death, estimate = .pred_class),
  type = "heatmap"
) +
  scale_fill_gradient(low = "#FFA54F", high = "#8B4513") +
  ggtitle("Confusion Matrix – Logistic Regression") +
  theme_minimal(base_size = 14)

# metrics
tbl <- combined %>%
  dplyr::rename(
    Metric = .metric,
    SVM = SVM,
    `Logistic Regression` = LR
  ) %>%
  kable(format = "html", digits = 3,
        caption = "Confronto tra metriche SVM vs LR") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(1, bold = TRUE) %>%
  row_spec(0, background = "#f7f7f7")


p_svm | p_lr

combined %>%
  dplyr::rename(
    Metric = .metric,
    SVM = SVM,
    `Logistic Regression` = LR
  ) %>%
  kable("html", digits = 3, caption = "SVM vs LR") %>%
  kable_styling(full_width = FALSE,
                bootstrap_options = c("striped", "hover", "condensed")) %>%
  column_spec(1, bold = TRUE)
```

with very similar results, it is better to choose the easiest model: Logistic Regression

# Model interpretability

```{r}
#| echo: false
#| code-fold: false

# Fit on train set
final_lr_fit <- workflows::fit(final_lr, data = train)
final_svm_fit <- workflows::fit(final_svm, data = train)

# Extract models
lr_model <- final_lr_fit %>% extract_fit_parsnip()
svm_model <- final_svm_fit %>% extract_fit_parsnip()

# Predictions on train
train_baked <- bake(prep(svm_rec, training = train), new_data = train)

# Logistic Regression
lr_probs <- predict(final_lr_fit, new_data = train, type = "prob")$.pred_1
lr_class <- ifelse(lr_probs >= 0.5, 1, 0)

# SVM 
svm_predict_fun <- function(model, newdata) {
  predict(model$fit, as.matrix(newdata), type = "probabilities")[,2]
}
svm_probs <- svm_predict_fun(svm_model, train_baked %>% select(-death))
svm_class <- ifelse(svm_probs >= 0.5, 1, 0)

# Add predictions to dataframe
train_preds <- train_baked %>%
  mutate(
    lr_prob = lr_probs,
    lr_class = factor(lr_class, levels = c(0,1)),
    svm_prob = svm_probs,
    svm_class = factor(svm_class, levels = c(0,1)),
    death = factor(death, levels = c(0,1)),
    lr_case = case_when(
      death == 1 & lr_class == 1 ~ "TP",
      death == 0 & lr_class == 0 ~ "TN",
      death == 0 & lr_class == 1 ~ "FP",
      death == 1 & lr_class == 0 ~ "FN"
    ),
    svm_case = case_when(
      death == 1 & svm_class == 1 ~ "TP",
      death == 0 & svm_class == 0 ~ "TN",
      death == 0 & svm_class == 1 ~ "FP",
      death == 1 & svm_class == 0 ~ "FN"
    ),
    row_index = row_number()
  )

tp_lr <- train_preds %>% filter(lr_case == "TP") %>% pull(row_index)
tn_lr <- train_preds %>% filter(lr_case == "TN") %>% pull(row_index)
fp_lr <- train_preds %>% filter(lr_case == "FP") %>% pull(row_index)
fn_lr <- train_preds %>% filter(lr_case == "FN") %>% pull(row_index)

tp_svm <- train_preds %>% filter(svm_case == "TP") %>% pull(row_index)
tn_svm <- train_preds %>% filter(svm_case == "TN") %>% pull(row_index)
fp_svm <- train_preds %>% filter(svm_case == "FP") %>% pull(row_index)
fn_svm <- train_preds %>% filter(svm_case == "FN") %>% pull(row_index)
```

## Feature importance

LR
```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"
# Logistic Regression (LASSO)
lr_coefs <- coef(lr_model$fit, s = best_params_lr$penalty) %>%
  as.matrix() %>%
  as.data.frame()
lr_coefs$feature <- rownames(lr_coefs)
colnames(lr_coefs)[1] <- "coefficient"
lr_coefs <- lr_coefs[-1,]  # remove intercept

p_lr <- ggplot(lr_coefs, aes(x = reorder(feature, coefficient), y = coefficient)) +
  geom_bar(stat = "identity", fill = "#FFA54F") +
  coord_flip() +
  ggtitle("Coefficients - Logistic Regression") +
  theme_minimal()
```

SVM
```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"
predictor_svm <- Predictor$new(
  model = svm_model$fit,
  data = train_baked %>% select(-death),
  y = as.numeric(train_baked$death),
  predict.fun = function(model, newdata) predict(model, as.matrix(newdata), type = "probabilities")[,2]
)

shap_list <- lapply(1:min(20, nrow(train_baked)), function(i) {
  Shapley$new(predictor_svm, x.interest = train_baked[i,] %>% select(-death))
})

shap_values_all <- do.call(rbind, lapply(shap_list, function(x) x$results))

p_svm <- ggplot(shap_values_all, aes(x = phi, y = reorder(feature, abs(phi)), color = phi)) +
  geom_violin(fill = "#458B74", alpha = 0.7, color = "#458B74") +
  labs(
    title = "Bee Swarm Plot of SHAP Values - SVM",
    x = "SHAP Value",
    y = "Feature",
    color = "SHAP Value"
  ) +
  theme_minimal(base_size = 14)

```

```{r,echo=FALSE}
p_lr + p_svm
```
#---------------
## Shapley values

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

svm_predict_fun <- function(model, newdata) {
  predict(model, newdata, type = "prob")[, 2]
}

svm_pred <- Predictor$new(
  model = svm_fit_baked,
  data  = svm_train_baked,
  y     =  svm_train_baked$death,
  predict.function = svm_predict_fun)

obs_index <- c(2335, 2700, 4199, 7005)

# Calculate Shapley values
shap_list <- lapply(obs_index, function(i) {
  x_test <- svm_train_baked[i,]
  shap <- Shapley$new(svm_pred, x.interest = x_test)
  df <- shap$results
  df$obs_id <- i
  df
})

shap_df <- dplyr::bind_rows(shap_list)

ggplot(shap_df, aes(x = feature, y = phi, fill = phi > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_fill_manual(values = c("TRUE" = "#EED2EE", "FALSE" = "#40E0D0")) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Shapley values – SVM",
    x = "Features",
    y = "Contribution (phi)"
  ) +
  facet_wrap(~ obs_id, ncol = 2)

```



```{r}
## Shapley values – Logistic Regression
#| echo: true
#| code-fold: true
#| code-summary: "code"

lr_pred <- Predictor$new(
  model = lr_fit_baked,
  data  = svm_train_baked,
  y     = svm_train_baked$death,
  predict.function = svm_predict_fun
)

obs_index <- c(2335, 2700, 4199, 7005)

# Calculate Shapley values
shap_list_lr <- lapply(obs_index, function(i) {
  x_test <- svm_train_baked 
  shap <- Shapley$new(lr_pred, x.interest = x_test)
  df <- shap$results
  df$obs_id <- i
  df
})

shap_df_lr <- dplyr::bind_rows(shap_list_lr)
# Visualizzazione
ggplot(shap_df_lr, aes(x = feature, y = phi, fill = phi > 0)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_fill_manual(values = c("TRUE" = "#EED2EE", "FALSE" = "#40E0D0")) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Shapley values – Logistic Regression",
    x = "Features",
    y = "Contribution (phi)"
  ) +
  facet_wrap(~ obs_id, ncol = 2)

```

## -----LIME

```{r}
#| echo: true
#| code-fold: true
#| code-summary: "code"

```

## -----Clustering on Shapley values
